# -*- coding: utf-8 -*-
"""Walmart Data Analysis and Forcasting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wf1XRJg6PqVrCt9Rs20v38BUUC2Ai9hf
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Set style
plt.style.use('default')
sns.set_palette("husl")

# Load the data
df = pd.read_csv('Walmart Data Analysis and Forcasting.csv')

# Convert Date to datetime
df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')

print("Dataset shape:", df.shape)
print("\nFirst 5 rows:")
print(df.head())

print("=== DATA CLEANING & PREPARATION ===")

# Check for missing values
print("Missing values:")
print(df.isnull().sum())

# Check data types
print("\nData types:")
print(df.dtypes)

# Check for duplicates
print(f"\nDuplicate rows: {df.duplicated().sum()}")

# Check unique stores
print(f"Unique stores: {df['Store'].nunique()}")
print(f"Store IDs: {sorted(df['Store'].unique())}")

# Basic statistics
print("\nBasic statistics:")
print(df.describe())

print("=== SALES TRENDS OVER TIME ===")

# Monthly sales trends
df['YearMonth'] = df['Date'].dt.to_period('M')
monthly_sales = df.groupby('YearMonth')['Weekly_Sales'].sum().reset_index()
monthly_sales['YearMonth'] = monthly_sales['YearMonth'].astype(str)

plt.figure(figsize=(15, 6))
plt.plot(monthly_sales['YearMonth'], monthly_sales['Weekly_Sales'])
plt.title('Monthly Sales Trends (2010-2012)')
plt.xlabel('Month')
plt.ylabel('Total Weekly Sales')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Yearly comparison
df['Year'] = df['Date'].dt.year
yearly_sales = df.groupby('Year')['Weekly_Sales'].sum()
print("\nYearly Sales:")
print(yearly_sales)

# Seasonal analysis - by quarter
df['Quarter'] = df['Date'].dt.quarter
quarterly_sales = df.groupby(['Year', 'Quarter'])['Weekly_Sales'].mean().unstack()
print("\nQuarterly Average Sales:")
print(quarterly_sales)

print("=== STORE PERFORMANCE COMPARISON ===")

# Store performance analysis
store_performance = df.groupby('Store')['Weekly_Sales'].agg(['mean', 'sum', 'std']).sort_values('sum', ascending=False)
print("Store Performance Ranking (by total sales):")
print(store_performance.head(10))

plt.figure(figsize=(12, 6))
store_performance['sum'].plot(kind='bar')
plt.title('Total Sales by Store')
plt.xlabel('Store ID')
plt.ylabel('Total Sales')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Store sales distribution
plt.figure(figsize=(10, 6))
sns.boxplot(x='Store', y='Weekly_Sales', data=df)
plt.title('Sales Distribution by Store')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

print("=== HOLIDAY IMPACT ANALYSIS ===")

# Holiday vs non-holiday sales
holiday_sales = df.groupby('Holiday_Flag')['Weekly_Sales'].agg(['mean', 'count'])
print("Holiday vs Non-Holiday Sales:")
print(holiday_sales)

# Statistical test for holiday impact
from scipy import stats
holiday_data = df[df['Holiday_Flag'] == 1]['Weekly_Sales']
non_holiday_data = df[df['Holiday_Flag'] == 0]['Weekly_Sales']
t_stat, p_value = stats.ttest_ind(holiday_data, non_holiday_data)
print(f"\nT-test results: t-statistic = {t_stat:.2f}, p-value = {p_value:.4f}")

if p_value < 0.05:
    print("Holidays have a statistically significant impact on sales")
else:
    print("No significant difference in sales between holiday and non-holiday weeks")

# Monthly holiday impact
df['Month'] = df['Date'].dt.month
monthly_holiday_impact = df.groupby(['Month', 'Holiday_Flag'])['Weekly_Sales'].mean().unstack()
monthly_holiday_impact['Difference'] = monthly_holiday_impact[1] - monthly_holiday_impact[0]
print("\nMonthly Holiday Impact (Average Sales Difference):")
print(monthly_holiday_impact[['Difference']].sort_values('Difference', ascending=False))

print("=== EXTERNAL FACTORS IMPACT ===")

# Correlation analysis
correlation_matrix = df[['Weekly_Sales', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment']].corr()
print("Correlation Matrix:")
print(correlation_matrix['Weekly_Sales'].sort_values(ascending=False))

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Matrix')
plt.tight_layout()
plt.show()

# Temperature impact
plt.figure(figsize=(12, 6))
plt.scatter(df['Temperature'], df['Weekly_Sales'], alpha=0.5)
plt.title('Temperature vs Weekly Sales')
plt.xlabel('Temperature')
plt.ylabel('Weekly Sales')
plt.tight_layout()
plt.show()

# Fuel price impact
plt.figure(figsize=(12, 6))
plt.scatter(df['Fuel_Price'], df['Weekly_Sales'], alpha=0.5)
plt.title('Fuel Price vs Weekly Sales')
plt.xlabel('Fuel Price')
plt.ylabel('Weekly Sales')
plt.tight_layout()
plt.show()

print("=== TIME SERIES ANALYSIS ===")

# Prepare data for time series
time_series_data = df.groupby('Date')['Weekly_Sales'].sum().reset_index()
time_series_data = time_series_data.set_index('Date')

# Decompose time series
from statsmodels.tsa.seasonal import seasonal_decompose

result = seasonal_decompose(time_series_data['Weekly_Sales'], model='additive', period=52)
result.plot()
plt.tight_layout()
plt.show()

# Check stationarity
from statsmodels.tsa.stattools import adfuller

adf_test = adfuller(time_series_data['Weekly_Sales'])
print(f"ADF Statistic: {adf_test[0]}")
print(f"p-value: {adf_test[1]}")
print("Critical Values:")
for key, value in adf_test[4].items():
    print(f"   {key}: {value}")

print("=== STORE CLUSTERING ANALYSIS ===")

# Create store profiles
store_profiles = df.groupby('Store').agg({
    'Weekly_Sales': ['mean', 'std'],
    'Temperature': 'mean',
    'Fuel_Price': 'mean',
    'CPI': 'mean',
    'Unemployment': 'mean'
}).round(2)

store_profiles.columns = ['Avg_Sales', 'Sales_Std', 'Avg_Temp', 'Avg_Fuel_Price', 'Avg_CPI', 'Avg_Unemployment']
print("Store Profiles:")
print(store_profiles.head())

# Normalize for clustering
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

scaler = StandardScaler()
scaled_data = scaler.fit_transform(store_profiles)

# Find optimal clusters using elbow method
inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(10, 6))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal Clusters')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.tight_layout()
plt.show()

# Apply K-means clustering
kmeans = KMeans(n_clusters=3, random_state=42)
store_profiles['Cluster'] = kmeans.fit_predict(scaled_data)

print("\nStore Clusters:")
print(store_profiles['Cluster'].value_counts())

# Analyze cluster characteristics
cluster_analysis = store_profiles.groupby('Cluster').mean()
print("\nCluster Characteristics:")
print(cluster_analysis)

print("=== COMPREHENSIVE SUMMARY ===")

print("1. Data Quality: No missing values, clean dataset")
print(f"2. Time Period: {df['Date'].min()} to {df['Date'].max()}")
print(f"3. Number of Stores: {df['Store'].nunique()}")
print(f"4. Total Sales: ${df['Weekly_Sales'].sum():,.2f}")
print(f"5. Average Weekly Sales: ${df['Weekly_Sales'].mean():,.2f}")
print(f"6. Holiday Impact: {holiday_sales.loc[1, 'mean']/holiday_sales.loc[0, 'mean']-1:.1%} higher sales on holidays")
print("7. Key Correlations:")
print("   - CPI: Moderate positive correlation with sales")
print("   - Unemployment: Moderate negative correlation with sales")
print("   - Temperature/Fuel Price: Weak correlations with sales")
print("8. Store Performance: Significant variation between stores")
print("9. Time Series: Clear seasonal patterns detected")
print("10. Clustering: Stores can be grouped into 3 distinct clusters")